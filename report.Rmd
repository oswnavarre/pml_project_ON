---
title: "Practical Machine Learning Project"
author: "Oswaldo Navarrete"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

This is the final project for Coursera’s Practical Machine Learning course which is part of the Data Science: Statistics and Machine Learning Specialization.

The data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants were used to predict the manner in which the participants did the exercise. The variable predicted is the “classe” variable in the training set. The training data set was partitioned 70\% for train the models, and 30\% for validate the models. 4 models were trained: Random Forest, Decision Tree,  and Gradient Boosted Trees using 3-folds cross validation on the training set. After training the models, predictions were done using the validation set to get the accuracy and the out of sample error rate (OOS). Finally, the model with the best accuracy and OOS was used to predict the 20 cases using the testing file.

## Used packages

```{r , message=F}
library(caret)
library(dplyr)
library(tidyr)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(skimr)
library(kableExtra)
```

## Loading data

```{r}
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

## Preprocessing the data

### Removing NAs

As part of the preprocessing, it was first determined how many variables had NAs and how many NAs had, was determined that 67 variables had 19216 NAs. After the columns that had less of 5\% of NAs were retained. In other words, the columns with more than 95% of NAs were eliminated.

```{r}
table(skim(training)$n_missing)

training <- training %>% 
  select(which(colMeans(is.na(.)) < 0.05))
```
### Near Zero Variance and irrelevant columns

The variables with near zero variance were removed. Finally the columns from 1 to 5 were removed because they had irrelevant information to creating the models.

```{r}
nzv <- nearZeroVar(training)
training <- training[-nzv]
```

```{r}
training <- training[-c(1:5)]
```

### Splitting the data

The training set was splitted in two sets, one to train the  models and another to validate them. 

```{r}
set.seed(12345)
inTrain <- createDataPartition(y=training$classe, p=0.7, list=F)
train <- training[inTrain,]
valid <- training[-inTrain,]
```

## Creating, validating and testing the models.

3 models were used: Random Forest, Decision Trees and Gradiend Boosted Trees. It is possible that more models could be tested, however due to project limitations only the models mentioned above will be compared.

3-fold cross validation was used for training the models. 

```{r}
control <- trainControl(method="cv", number=3, verboseIter=F)
```


### Random Forest

#### Model

```{r}
model1 <- train(classe ~ ., data=train, method="rf", trControl = control)
model1$finalModel
```

#### Prediction, Confusion Matrix and Accuracy

```{r}
pred_model1 <- predict(model1, valid)
confmat_model1 <- confusionMatrix(pred_model1, factor(valid$classe))
confmat_model1$overall["Accuracy"]
```
### Decision Tree

#### Model 

```{r}
model2 <- train(classe~., data=train, method="rpart", trControl = control, tuneLength = 5)
fancyRpartPlot(model2$finalModel)
```

#### Prediction, Confusion Matrix and Accuracy

```{r}
pred_model2 <- predict(model2, valid)
confmat_model2 <- confusionMatrix(pred_model2, factor(valid$classe))
confmat_model2$overall["Accuracy"]
```

### Gradient Boosted Trees

#### Model

```{r}
model3 <- train(classe~., data=train, method="gbm", trControl = control, tuneLength = 5, verbose = F)
model3$finalModel
```

#### Prediction, Confusion Matrix and Accuracy

```{r}
pred_model3 <- predict(model3, valid)
confmat_model3 <- confusionMatrix(pred_model3, factor(valid$classe))
confmat_model3$overall["Accuracy"]
```

### Accuracy and Out of Sample Error of all models

```{r}
accs <- data.frame(
  Model = c("model1","model2","model3"),
  Accuracy = c(confmat_model1$overall["Accuracy"],
               confmat_model2$overall["Accuracy"],
               confmat_model3$overall["Accuracy"])
)

accs <- accs %>%
  mutate(
    Accuracy = round(Accuracy,4),
    OOS = 1 -Accuracy )
accs
```

The best model was the gradient boosted tree with an accuracy of `r round(confmat_model3$overall["Accuracy"],4)` and an out of sample error rate of `r round(1-confmat_model3$overall["Accuracy"],4)`. This model will be used for test set. 

## Predictions using test set

```{r}
final_pred <- predict(model3, testing)
preds <- data.frame(
  Case = paste("Case", seq(1,20,1)),
  Prediction = final_pred
)
kable(preds)
```

## Another plots

Plots of models.

```{r}
plot(model1)
```

```{r}
plot(model2)
```
```{r}
plot(model3)
```

